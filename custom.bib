% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{AV_HuBERT,
  title     = {Learning {Audio}-{Visual} {Speech} {Representation} by {Masked} {Multimodal} {Cluster} {Prediction}},
  url       = {http://arxiv.org/abs/2201.02184},
  abstract  = {Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5\% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6\%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9\% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40\% relative WER reduction over the state-of-the-art performance (1.3\% vs 2.3\%). Our code and models are available at https://github.com/facebookresearch/av\_hubert},
  urldate   = {2023-12-09},
  publisher = {arXiv},
  author    = {Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  month     = mar,
  year      = {2022},
  note      = {arXiv:2201.02184 [cs, eess]},
  keywords  = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound}
}

@inproceedings{denoising-autoencoder,
  author  = {Vincent, Pascal and Larochelle, Hugo and Bengio, Y. and Manzagol, Pierre-Antoine},
  year    = {2008},
  month   = {01},
  pages   = {1096-1103},
  title   = {Extracting and composing robust features with denoising autoencoders},
  journal = {Proceedings of the 25th International Conference on Machine Learning},
  doi     = {10.1145/1390156.1390294}
}

@inproceedings{HMMs,
  author    = {Bahl, L. and Brown, P. and de Souza, P. and Mercer, R.},
  booktitle = {ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title     = {Maximum mutual information estimation of hidden Markov model parameters for speech recognition},
  year      = {1986},
  volume    = {11},
  number    = {},
  pages     = {49-52},
  doi       = {10.1109/ICASSP.1986.1169179}
}

@inproceedings{clustering,
  author    = {Rabiner, L. and Wilpon, J.},
  booktitle = {ICASSP '79. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  title     = {Considerations in applying clustering techniques to speaker independent word recognition},
  year      = {1979},
  volume    = {4},
  number    = {},
  pages     = {578-581},
  doi       = {10.1109/ICASSP.1979.1170822}
}

@inproceedings{glove,
  title     = {{G}lo{V}e: Global Vectors for Word Representation},
  author    = {Pennington, Jeffrey  and
               Socher, Richard  and
               Manning, Christopher},
  editor    = {Moschitti, Alessandro  and
               Pang, Bo  and
               Daelemans, Walter},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D14-1162},
  doi       = {10.3115/v1/D14-1162},
  pages     = {1532--1543}
}

@misc{speecht5,
  title      = {{SpeechT5}: {Unified}-{Modal} {Encoder}-{Decoder} {Pre}-{Training} for {Spoken} {Language} {Processing}},
  shorttitle = {{SpeechT5}},
  url        = {http://arxiv.org/abs/2110.07205},
  doi        = {10.48550/arXiv.2110.07205},
  abstract   = {Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder. Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder. Extensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification. We release our code and model at https://github.com/microsoft/SpeechT5.},
  urldate    = {2023-12-09},
  publisher  = {arXiv},
  author     = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},
  month      = may,
  year       = {2022},
  note       = {arXiv:2110.07205 [cs, eess]},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound}
}

@misc{t5,
  title     = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
  url       = {http://arxiv.org/abs/1910.10683},
  abstract  = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  urldate   = {2023-12-09},
  publisher = {arXiv},
  author    = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  month     = sep,
  year      = {2023},
  note      = {arXiv:1910.10683 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning}
}

@misc{shi_learning_2022-1,
  title     = {Learning {Audio}-{Visual} {Speech} {Representation} by {Masked} {Multimodal} {Cluster} {Prediction}},
  url       = {http://arxiv.org/abs/2201.02184},
  abstract  = {Video recordings of speech contain correlated audio and visual information, providing a strong signal for speech representation learning from the speaker's lip movements and the produced sound. We introduce Audio-Visual Hidden Unit BERT (AV-HuBERT), a self-supervised representation learning framework for audio-visual speech, which masks multi-stream video input and predicts automatically discovered and iteratively refined multimodal hidden units. AV-HuBERT learns powerful audio-visual speech representation benefiting both lip-reading and automatic speech recognition. On the largest public lip-reading benchmark LRS3 (433 hours), AV-HuBERT achieves 32.5\% WER with only 30 hours of labeled data, outperforming the former state-of-the-art approach (33.6\%) trained with a thousand times more transcribed video data (31K hours). The lip-reading WER is further reduced to 26.9\% when using all 433 hours of labeled data from LRS3 and combined with self-training. Using our audio-visual representation on the same benchmark for audio-only speech recognition leads to a 40\% relative WER reduction over the state-of-the-art performance (1.3\% vs 2.3\%). Our code and models are available at https://github.com/facebookresearch/av\_hubert},
  urldate   = {2023-12-09},
  publisher = {arXiv},
  author    = {Shi, Bowen and Hsu, Wei-Ning and Lakhotia, Kushal and Mohamed, Abdelrahman},
  month     = mar,
  year      = {2022},
  note      = {arXiv:2201.02184 [cs, eess]},
  keywords  = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound}
}

@article{srl-review,
  title      = {Self-{Supervised} {Speech} {Representation} {Learning}: {A} {Review}},
  volume     = {16},
  issn       = {1932-4553, 1941-0484},
  shorttitle = {Self-{Supervised} {Speech} {Representation} {Learning}},
  url        = {http://arxiv.org/abs/2205.10643},
  doi        = {10.1109/JSTSP.2022.3207050},
  abstract   = {Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.},
  number     = {6},
  urldate    = {2023-12-09},
  journal    = {IEEE Journal of Selected Topics in Signal Processing},
  author     = {Mohamed, Abdelrahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob D. and Edin, Joakim and Igel, Christian and Kirchhoff, Katrin and Li, Shang-Wen and Livescu, Karen and Maaløe, Lars and Sainath, Tara N. and Watanabe, Shinji},
  month      = oct,
  year       = {2022},
  note       = {arXiv:2205.10643 [cs, eess]},
  keywords   = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
  pages      = {1179--1210}
}

@article{zhu_vatlm:_2023,
  title      = {{VATLM}: {Visual}-{Audio}-{Text} {Pre}-{Training} with {Unified} {Masked} {Prediction} for {Speech} {Representation} {Learning}},
  issn       = {1520-9210, 1941-0077},
  shorttitle = {{VATLM}},
  url        = {http://arxiv.org/abs/2211.11275},
  doi        = {10.1109/TMM.2023.3275873},
  abstract   = {Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.},
  urldate    = {2023-12-09},
  journal    = {IEEE Transactions on Multimedia},
  author     = {Zhu, Qiushi and Zhou, Long and Zhang, Ziqiang and Liu, Shujie and Jiao, Binxing and Zhang, Jie and Dai, Lirong and Jiang, Daxin and Li, Jinyu and Wei, Furu},
  year       = {2023},
  note       = {arXiv:2211.11275 [cs, eess]},
  keywords   = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Sound},
  pages      = {1--11}
}

@article{unsupervised_learning,
  author  = {Kemp, Thomas and Waibel, Alex},
  year    = {1970},
  month   = {02},
  pages   = {},
  title   = {Unsupervised Training Of A Speech Recognizer: Recent Experiments},
  volume  = {6},
  journal = {Proc Eurospeech Budapest}
}