# Plan for Presentation and Paper: Multi-modal Speech Representation Learning

1. Create outline for paper

## Papers

[1] Mohamed, Abdelrahman, et al. "Self-supervised speech representation learning: A review." IEEE Journal of Selected Topics in Signal Processing (2022).
Intrinsic vs. Extrinsic Approaches
Questions:

[2] Ao, Junyi, et al. "Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing." ACL (2022).

Questions:

[3] Zhu, Qiushi, et al. "Vatlm: Visual-audio-text pre-training with unified masked prediction for speech representation learning." IEEE Transactions on Multimedia (2023).
-

Questions:

[4] Shi, Bowen, et al. "Learning audio-visual speech representation by masked multimodal cluster prediction." ICLR (2022).

Read from Experiment

## What needs to be in the presentation


0. What is Speech Representation Learning, why do we need it? (Review Section VI), What is Self-Supervised Learning
1. History of (Speech) Representation Learning?
2. Motivation: Current state (is single mode current sota?), then show that multi modal datas is easy to get (if it's true)
3. Pre-Training and Fine-Tuning tasks 
4. Requirements of Speech Representations
5. SPEECH REPRESENTATION LEARNING PARADIGMS
6. Multi-modal approaches, challenges thereof
7. Gathering multi-modal data
8. Evaluation methods
9. AV HuBERT
10. SpeechT5
11. VAT LM
12. Advantages / Drawbacks of Multi-modal learning (e.g., domain-specific data)
13. Present their results? Possibly better scores / more use cases. What's the current SOTA in VSR/AVSR/ASR?
14. Challenges and Prospects

