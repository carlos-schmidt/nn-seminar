% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[]{ACL2023}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{placeins}
\usepackage[bb=dsserif]{mathalpha}

\newcommand{\enquote}[1]{``#1''}

\title{Neural Network Seminar\\Multi-modal Speech Representation Learning}
    
\author{Carlos Schmidt \\\texttt{uuwss@student.kit.edu}\\}

\begin{document}
\maketitle
\begin{abstract}
While speech representation learning is a well-known approach to train versatile models capable of solving different natural language processing (NLP) tasks like automatic or visual speech recognition (ASR/VSR), a relatively new path towards leveraging multiple modalities shows promising downstream performance gains. Furthermore, multi-modal data like video with sound is everywhere and because of their intuitiveness easy to produce, e.g., by recording a video of a person speaking. The reviewed models in this paper exploit different combinations of three types of modalities: audio, video and textual data. The performance gain of these models with respect to previous single-mode approaches is shown as well as differences between the models themselves. Overall, multi-modal approaches to speech representation learning demonstrate enormous potential and have already set new benchmarks for numerous NLP tasks.
\end{abstract}

\section{Introduction}
\input{chapters/intro}
\section{Single-mode Speech Representation Models}\label{sec:single-mode}
In this section, we briefly discuss two different approaches for single-mode speech representation models. Both approaches use audio data to learn representations and can outperform previous supervised models in tasks such as automatic speech recognition using the same amount of training data \cite{wav2vec20}. A more comprehensive list of single-mode approaches to speech representations can be found in \citealt{srl-review}.
\subsection{wav2vec2.0}
\input{chapters/single-mode/wav2vec}
\subsection{HuBERT}\label{sec:HuBERT}
\input{chapters/single-mode/hubert}

\section{Multi-modal Speech Representation Models}
This section presents three examples of speech representation learning models, each leveraging a different combination of input channels. First, we will look at SpeechT5 \cite{speecht5}, which uses additionally to the audio modality the text modality for learning speech representations. Next, we will take a look at AV-HuBERT \cite{AV_HuBERT}, which is an extension of the single-mode model HuBERT \cite{hubert} and benefits from the audio modality as well as the video modality. Lastly, VAT-LM \cite{vatlm} will be covered, leveraging all three mentioned modalities: audio, video and text.
\subsection{SpeechT5}\label{sec:speecht5}
\input{chapters/multi-mode/speecht5}
\subsection{AV-HuBERT}\label{sec:av-hubert}
\input{chapters/multi-mode/av-hubert}
\subsection{VAT-LM}
\input{chapters/multi-mode/vat-lm}
\section{Discussion}
\input{chapters/discussion}


% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\end{document}
