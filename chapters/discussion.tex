By leveraging a bigger set of modalities than previous, single-mode, speech representation learning approaches, the presented multi-modal models are able to achieve higher accuracy and lower word error rates for several tasks like visual speech recognition, as we showed in this review. The reason for this is that even for single-mode tasks like automatic speech recognition, the additional channels used for the pre-training of these models help enriching the speech representations and make them more robust, because they all share the common concept of the language and speech. Another advantage of multi-mode models is that they often require less labeled data than single-mode approaches as we have seen in \ref{sec:av-hubert}. Nonetheless, the performance comparisons may not all be perfectly representative since technically more data was used to train the multi-modal models. Other challenges in the field of multi-modality speech representation learning include the available data typically being scarce and domain-specific, e.g., for the visual speech recognition task where a specific pose of the speaker is needed in the video frames for a network to detect the lip movements.