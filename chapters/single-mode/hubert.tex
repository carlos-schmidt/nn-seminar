The ``Hidden-Unit BERT'' (HuBERT) model also consists of a CNN followed by a transformer, specifically the BERT architecture~\cite{devlin2019bert}. The difference to the wav2vec2.0 approach is that HuBERT uses in its first iteration k-means clusters, and in further iterations clustered intermediate layer features of its transformer network as its learning targets. Furthermore, HuBERT does not employ a contrastive loss but rather uses the predictive and generative learning paradigms by predicting the clustered targets at masked positions of the model's output~\cite{hubert}.