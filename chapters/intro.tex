The goal of learning intermediate latent representations of a given input instead of learning an explicit task end-to-end is to be able to extract information out of the input and use this information to tackle multiple tasks. Speech representations are latent representation of speech input. This input could be in formats such as raw waveform, or mel filterbank features. Speech representations are related to (textual) semantic word embeddings as these embeddings are also latent representations with extracted information, in that case their semantic relatedness.~\cite{glove} The difference from textual embeddings to speech representations and what makes learning speech representations hard is that spoken words don't have clear boundaries, speech input is continuous as opposed to discrete textual words and that speech contains more information than text like for example speaker information, noise or emotion.\\The first attempts at learning speech representations were done using clustering algorithms like k-means or Gaussian Mixture Models~\cite{clustering} and later improved by adding Hidden Markov Models to allow processing of continuous speech rather than single words.~\cite{HMMs} Currently, the prevalent approach for learning speech representations is to perform pretext task optimization, also known as pre-training. In this approach, the representations are learned by solving a task that is derived by unlabeled data like for example predicting the next word in a sequence of words or predicting a masked word in a sequence. The advantage of this approach is that it only needs unlabeled data which is of higher availability than labeled data. Within the pretext task optimization approach, the following three learning paradigms can be considered the most widely used:
\begin{enumerate}
    \item \textit{Generative learning}: In generative learning, an input is reconstructed based on a limited view of it. This can mean predicting the next word in a sequence, a masked word within an input or predicting the original from a noisy input~\cite{denoising-autoencoder}.
    \item \textit{Contrastive learning}: Contrastive learning is based on the idea of\ldots
    \item \textit{Predictive learning}: Predictive learning is a\ldots
\end{enumerate}
This paper is structured as follows. First, single-mode speech representation models are introduced, specifically wav2wec2.0 and HuBERT which is the basis of some of the multi-modal speech representation models. Afterwards, multi-modal speech representation models are shown by first looking at their architecture, then the learning approach and specifics and finally the performance of the models is discussed with respect to other multi-modal and also single-mode models. Finally, a discussion about the impact, advantages and challenges of multi-modal speech representation learning is conducted including a comparison to single mode approaches.
