The SpeechT5 framework, first introduced in 2021 by Microsoft~\cite{speecht5}, is an expansion of the Text-to-Text Transfer Transformer (T5) framework~\cite{t5}. SpeechT5 is, as the name suggests, an audio and text approach to speech representations. 
\subsubsection{Model Architecture}
SpeechT5 is based on the encoder/decoder Transformer architecture also used in the original Transformer paper~\cite{vaswani2023attention}.

The encoder component takes as input the audio and text data which are previously encoded with pre-nets. For audio data, a pretrained wav2vec2.0 model is used as pre-net, while for text, word embeddings are used in the preprocessing step. In the actual Transformer Encoder, the preprocessed audio and text are input to produce representations for the decoder.

The decoder takes as input the representations produced by the encoder and outputs a sequence which is then fed into the post-processing mode-specific networks. For audio data, this is a log Melfilterbank predictor, and for the text output a representation-to-token transformation network which computes a probability distribution of tokens from the decoder's output. Finally, for some tasks like voice conversion, the input data is also fed into a decoder pre-net, which is a feed forward network with the log Mel-filterbank features of the input for audio and another embedding layer for text inputs.
\subsubsection{Training}
There are three types of pre-training for SpeechT5: Speech pre-training, text pre-training and joint pre-training. One of the goals of pre-training is ``to learn representations
capturing the modality-invariant information''~\cite{speecht5} for tasks such as ASR. The speech pre-training uses the generative tasks of bidirectional masked prediction and sequence-to-sequence generation with randomly masked portions of the input. The loss functions used are the cross-entropy loss for the masked prediction and the L1-loss for the generation task. For text pre-training, masked prediction is performed as well, where 30\% of the text is randomly masked. As a loss function, the maximum likelihood loss is used. Finally, the joint pre-training approach aims to make the audio and text representations (the output of the Transformer Encoder) capture ``modality invariant information''~\cite{speecht5}. This is achieved by using a shared codebook for both modalities. The Transformer Encoder representations are converted into codebook elements by finding the closest element, which is done with a nearest neighbor search over the L2 distance: $$c_i = \arg\mathop{\min}\limits_{j\in [K]} ||u_i-c_j||_2$$ Here, $u_i$ is the Transformer Encoder representation, $c_i$ is the target codebook entry and $K$ is the amount of codes in the codebook. Lastly, a diversity loss is used to encourage the encoder to use more codes by smoothing the distribution of the probabilities of each code being used.

For the fine-tuning process, combinations of pre- and post-nets are activated. For example, when fine-tuning for ASR, the speech-encoder pre-net and the text-decoder pre- and post-nets are used as well as the Transformer Encoder and Decoder. Then, the loss of the specific task is used to train the SpeechT5 model end-to-end.
\subsubsection{Performance}
In Table \ref{table:speecht5_performance}, a performance comparison between SpeechT5 and the single-mode approaches shown in Section \ref{sec:single-mode} is presented. The results show that SpeechT5 consistently achieves a lower word error rate (WER) than the previously covered single-mode approaches. While SpeechT5 does outperform those single-mode models, it is important to note that SpeechT5 uses an additional 400 million text sentences in the pre-training phase~\cite{speecht5}, which wav2vec2.0 and HuBERT cannot.

\begin{table}[htbp]
    \centering
    \begin{tabular}{l c c c}
    \hline
    \textbf{Model} & LM & test-clean & test-other\\
    \hline
    HuBERT&-&5.8&13.3\\
    wav2vec2.0&-&6.1&13.3\\
    SpeechT5&-&\textbf{4.4}&\textbf{10.4}\\
    \hline
    HuBERT&4-gram&3.4&8.1\\
    wav2vec2.0&Transf.&2.6&6.3\\
    SpeechT5&Transf.&\textbf{2.4}&\textbf{5.8}\\
    \hline
    \end{tabular}
    \caption{\label{table:speecht5_performance}ASR Performance on the 100 hours subset of LibriSpeech. (from \cite{speecht5})}
\end{table}